package com.datasift.dropwizard.kafka;

import com.datasift.dropwizard.kafka.producer.KafkaProducer;
import com.fasterxml.jackson.annotation.JsonProperty;
import io.dropwizard.util.Duration;
import io.dropwizard.util.Size;
import kafka.javaapi.producer.Producer;
import kafka.producer.ProducerConfig;
import org.hibernate.validator.constraints.NotEmpty;

import javax.validation.constraints.Max;
import javax.validation.constraints.Min;
import javax.validation.constraints.NotNull;
import java.util.Properties;

/**
 * Configuration and factory for a KafkaProducer.
 * <p/>
 * By default, the producer will be synchronous, blocking the calling thread until the message has
 * been sent.
 * <p/>
 */
public class KafkaProducerFactory {
    /**
     * metadata.broker.list
     * This is for bootstrapping and the producer will only use it for getting metadata (topics,
     * partitions and replicas). The socket connections for sending the actual data will be established
     * based on the broker information returned in the metadata. The format is host1:port1,host2:port2,
     * and the list can be a subset of brokers or a VIP pointing to a subset of brokers.
     *
     */
    @NotEmpty
    protected String metadataBrokerList = "localhost:9092";

    /**
     * request.required.acks
     * This value controls when a produce request is considered completed. Specifically, how many other
     * brokers must have committed the data to their log and acknowledged this to the leader? Typical
     * values are
     * 0, which means that the producer never waits for an acknowledgement from the broker (the same
     * behavior as 0.7). This option provides the lowest latency but the weakest durability guarantees
     * (some data will be lost when a server fails).
     * 1, which means that the producer gets an acknowledgement after the leader replica has received
     * the data. This option provides better durability as the client waits until the server
     * acknowledges the request as successful (only messages that were
     * written to the now-dead leader but not yet replicated will be lost).
     * -1, which means that the producer gets an acknowledgement after all in-sync replicas have
     * received the data. This option provides the best durability, we guarantee that no messages will
     * be lost as long as at least one in sync replica remains.
     */
    @Min(-1) @Max(1)
    protected int requestRequiredAcks = 0;

    /**
     * request.timeout.ms	10000
     * The amount of time the broker will wait trying to meet the request.required.acks requirement
     * before sending back an error to the client.
     */
    @NotNull
    protected Duration requestTimeout = Duration.milliseconds(10000);

    /**
     * producer.type	sync
     * This parameter specifies whether the messages are sent asynchronously in a background thread.
     * Valid values are (1) async for asynchronous send and (2) sync for synchronous send.
     * By setting the producer to async we allow batching together of requests (which is great for
     * throughput) but open the possibility of a failure of the client machine dropping unsent data.
     */
    @NotEmpty
    protected String producerType = "sync";

    /**
     * serializer.class	kafka.serializer.DefaultEncoder
     * The serializer class for messages. The default encoder takes a byte[] and returns
     * the same byte[].
     */
    @NotEmpty
    protected String serializerClass = "kafka.serializer.DefaultEncoder";

    /**
     * key.serializer.class
     * The serializer class for keys (defaults to the same as for messages if nothing is given).
     */
    @NotEmpty
    protected String keySerializerClass = "kafka.serializer.DefaultEncoder";

    /**
     * partitioner.class	kafka.producer.DefaultPartitioner	The partitioner class for partitioning
     * messages amongst sub-topics. The default partitioner is based on the hash of the key.
     */
    @NotEmpty
    protected String partitionerClass = "kafka.producer.DefaultPartitioner";

    /**
     * compression.codec	none
     * This parameter allows you to specify the compression codec for all data generated by
     * this producer. Valid values are "none", "gzip" and "snappy".
     */
    @NotEmpty
    protected String compressionCodec = "none";

    /**
     * compressed.topics	null
     * This parameter allows you to set whether compression should be turned on for particular topics.
     * If the compression codec is anything other than NoCompressionCodec, enable compression only for
     * specified topics if any. If the list of compressed topics is empty, then enable the specified
     * compression codec for all topics. If the compression codec is NoCompressionCodec, compression is
     * disabled for all topics
     */
    protected String compressedTopics = null;

    /**
     * message.send.max.retries	3
     * This property will cause the producer to automatically retry a failed send request.
     * This property specifies the number of retries when such failures occur. Note that setting a
     * non-zero value here can lead to duplicates in the case of network errors that cause a message to
     * be sent but the acknowledgement to be lost.
     */
    @Min(0)
    protected int messageSendMaxRetries = 3;

    /**
     * retry.backoff.ms	100
     * Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader
     * has been elected. Since leader election takes a bit of time, this property specifies the amount
     * of time that the producer waits before refreshing the metadata.
     */
    @NotNull
    protected Duration retryBackoff = Duration.milliseconds(100);

    /**
     * topic.metadata.refresh.interval.ms	600 * 1000
     * The producer generally refreshes the topic metadata from brokers when there is a failure
     * (partition missing, leader not available...). It will also poll regularly
     * (default: every 10min so 600000ms). If you set this to a negative value, metadata will
     * only getrefreshed on failure. If you set this to zero, the metadata will get refreshed
     * after each message sent (not recommended). Important note: the refresh happen only
     * AFTER the message is sent, so if the producer never sends a message the metadata is
     * never refreshed
     */
    @NotNull
    protected Duration topicMetadataRefreshInterval = Duration.minutes(10);

    /**
     * queue.buffering.max.ms	5000
     * Maximum time to buffer data when using async mode.
     * For example a setting of 100 will try to batch together 100ms of messages to send at once.
     * This will improve throughput but adds message delivery latency due to the buffering.
     */
    @NotNull
    protected Duration queueBufferingMax = Duration.seconds(5);

    /**
     * queue.buffering.max.messages	10000
     * The maximum number of unsent messages that can be
     * queued up the producer when using async mode before either the producer must be blocked
     * or data must be dropped.
     */
    @Min(0)
    protected int queueBufferingMaxMessages = 10000;

    /**
     * queue.enqueue.timeout.ms	-1
     * The amount of time to block before dropping messages when running in async mode and the buffer
     * has reached queue.buffering.max.messages. If set to 0 events will be enqueued immediately or
     * dropped if the queue is full (the producer send call will never block). If set to -1 the producer
     * will block indefinitely and never willingly drop a send.
     */
    // Leaving this as an int not a Duration because of significance of magic values (0, -1)
    // Ick.
    @Min(-1)
    protected int queueEnqueueTimeoutMillisecs = -1;

    /**
     * batch.num.messages	200	The number of messages to send in one batch when using async mode. The
     * producer will wait until either this number of messages are ready to send or queue.buffer.max.ms
     * is reached.
     */
    @Min(1)
    protected int batchNumMessages = 200;

    /**
     * send.buffer.bytes	100 * 1024
     * Socket write buffer size
     */
    @NotNull
    protected Size sendBufferSize = Size.kilobytes(100);

    /**
     * client.id	""	The client id is a user-specified string sent in each request to help
     * trace calls. It should logically identify the application making the request.
     */
    @NotNull
    protected String clientId = "";

    @JsonProperty
    public String getMetadataBrokerList() {
        return metadataBrokerList;
    }

    @JsonProperty
    public void setMetadataBrokerList(String metadataBrokerList) {
        this.metadataBrokerList = metadataBrokerList;
    }

    @JsonProperty
    public int getRequestRequiredAcks() {
        return requestRequiredAcks;
    }

    @JsonProperty
    public void setRequestRequiredAcks(int requestRequiredAcks) {
        this.requestRequiredAcks = requestRequiredAcks;
    }

    @JsonProperty
    public Duration getRequestTimeout() {
        return requestTimeout;
    }

    @JsonProperty
    public void setRequestTimeout(Duration requestTimeout) {
        this.requestTimeout = requestTimeout;
    }

    @JsonProperty
    public String getProducerType() {
        return producerType;
    }

    @JsonProperty
    public void setProducerType(String producerType) {
        this.producerType = producerType;
    }

    @JsonProperty
    public String getSerializerClass() {
        return serializerClass;
    }

    @JsonProperty
    public void setSerializerClass(String serializerClass) {
        this.serializerClass = serializerClass;
    }

    @JsonProperty
    public String getKeySerializerClass() {
        return keySerializerClass;
    }

    @JsonProperty
    public void setKeySerializerClass(String keySerializerClass) {
        this.keySerializerClass = keySerializerClass;
    }

    @JsonProperty
    public String getPartitionerClass() {
        return partitionerClass;
    }

    @JsonProperty
    public void setPartitionerClass(String partitionerClass) {
        this.partitionerClass = partitionerClass;
    }

    @JsonProperty
    public String getCompressionCodec() {
        return compressionCodec;
    }

    @JsonProperty
    public void setCompressionCodec(String compressionCodec) {
        this.compressionCodec = compressionCodec;
    }

    @JsonProperty
    public String getCompressedTopics() {
        return compressedTopics;
    }

    @JsonProperty
    public void setCompressedTopics(String compressedTopics) {
        this.compressedTopics = compressedTopics;
    }

    @JsonProperty
    public int getMessageSendMaxRetries() {
        return messageSendMaxRetries;
    }

    @JsonProperty
    public void setMessageSendMaxRetries(int messageSendMaxRetries) {
        this.messageSendMaxRetries = messageSendMaxRetries;
    }

    @JsonProperty
    public Duration getRetryBackoff() {
        return retryBackoff;
    }

    @JsonProperty
    public void setRetryBackoff(Duration retryBackoff) {
        this.retryBackoff = retryBackoff;
    }

    @JsonProperty
    public Duration getTopicMetadataRefreshInterval() {
        return topicMetadataRefreshInterval;
    }

    @JsonProperty
    public void setTopicMetadataRefreshInterval(Duration topicMetadataRefreshInterval) {
        this.topicMetadataRefreshInterval = topicMetadataRefreshInterval;
    }

    @JsonProperty
    public Duration getQueueBufferingMax() {
        return queueBufferingMax;
    }

    @JsonProperty
    public void setQueueBufferingMax(Duration queueBufferingMax) {
        this.queueBufferingMax = queueBufferingMax;
    }

    @JsonProperty
    public int getQueueBufferingMaxMessages() {
        return queueBufferingMaxMessages;
    }

    @JsonProperty
    public void setQueueBufferingMaxMessages(int queueBufferingMaxMessages) {
        this.queueBufferingMaxMessages = queueBufferingMaxMessages;
    }

    @JsonProperty
    public int getQueueEnqueueTimeoutMillisecs() {
        return queueEnqueueTimeoutMillisecs;
    }

    @JsonProperty
    public void setQueueEnqueueTimeoutMillisecs(int queueEnqueueTimeoutMillisecs) {
        this.queueEnqueueTimeoutMillisecs = queueEnqueueTimeoutMillisecs;
    }

    @JsonProperty
    public int getBatchNumMessages() {
        return batchNumMessages;
    }

    @JsonProperty
    public void setBatchNumMessages(int batchNumMessages) {
        this.batchNumMessages = batchNumMessages;
    }

    @JsonProperty
    public Size getSendBufferSize() {
        return sendBufferSize;
    }

    @JsonProperty
    public void setSendBufferSize(Size sendBufferSize) {
        this.sendBufferSize = sendBufferSize;
    }

    @JsonProperty
    public String getClientId() {
        return clientId;
    }

    @JsonProperty
    public void setClientId(String clientId) {
        this.clientId = clientId;
    }

    public ProducerConfig asProducerConfig() {
        Properties props = new Properties();
        props.put("metadata.broker.list", getMetadataBrokerList());
        props.put("request.required.acks", Integer.toString(getRequestRequiredAcks()));
        props.put("request.timeout.ms", Long.toString(getRequestTimeout().toMilliseconds()));
        props.put("producer.type", getProducerType());
        props.put("serializer.class", getSerializerClass());
        props.put("key.serializer.class", getKeySerializerClass());
        props.put("partitioner.class", getPartitionerClass());
        props.put("compression.codec", getCompressionCodec());

        if (compressedTopics != null) {
            props.put("compressed.topics", getCompressedTopics());
        }
        props.put("message.send.max.retries", Integer.toString(getMessageSendMaxRetries()));
        props.put("retry.backoff.ms",
                Long.toString(getRetryBackoff().toMilliseconds()));
        props.put("topic.metadata.refresh.interval.ms",
                Long.toString(getTopicMetadataRefreshInterval().toMilliseconds()));
        props.put("queue.buffering.max.ms",
                Long.toString(getQueueBufferingMax().toMilliseconds()));
        props.put("queue.buffering.max.messages", Integer.toString(getQueueBufferingMaxMessages()));
        props.put("queue.enqueue.timeout.ms", Integer.toString(getQueueEnqueueTimeoutMillisecs()));
        props.put("batch.num.messages", Integer.toString(getBatchNumMessages()));
        props.put("send.buffer.bytes", Long.toString(getSendBufferSize().toBytes()));
        props.put("client.id", getClientId());

        return new ProducerConfig(props);
    }

    public <K,V> KafkaProducer<K,V> build() {
        return new KafkaProducer<K,V>(new Producer<K,V>(asProducerConfig()));
    }
}
